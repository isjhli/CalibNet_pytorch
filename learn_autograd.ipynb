{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自动求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([12., 18.]) tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([2., 3.], requires_grad=True)\n",
    "b = torch.tensor([6., 4.], requires_grad=True)\n",
    "Q = 3 * a ** 2 - b ** 2\n",
    "'''\n",
    "    需要在 Q.backward() 中显式传入一个 gradient 参数，因为它是向量。\n",
    "    gradient 是一个与 Q 形状相同的张量，它表示 Q 相对于自身的梯度 dQ/dQ=1\n",
    "'''\n",
    "# Q.backward(torch.tensor([1., 1.]))\n",
    "'''\n",
    "    等效地，也可以将 Q 聚合为标量并隐式调用反向传播，即 Q.sum().backward()\n",
    "'''\n",
    "Q.sum().backward()\n",
    "\n",
    "'''\n",
    "    梯度存储在 a.grad 和 b.grad 中\n",
    "'''\n",
    "print(a.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义自动求导"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = k * func1(c * x) + b，其中func1(p)=p^2，dfunc1/dp = 2p\n",
    "\n",
    "求导\n",
    "\n",
    "    dk = (c * x) ^ 2\n",
    "\n",
    "    db = 1\n",
    "\n",
    "    dc = k * 2(c * x) * x\n",
    "\n",
    "自定义：\n",
    "    dfunc1/dp = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14.4000)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class fun1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):  # 必须有 ctx，ctx 表示上下文管理器，一般用来保存在 backward 阶段会用到的 tensor\n",
    "        ctx.save_for_backward(input)\n",
    "        out = input ** 2  # 定义反向传播函数 fun1 = input ** 2\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):  # grad_output 为上一层累积的梯度（求导结果）\n",
    "        input, = ctx.saved_tensors  # 调用 ctx 获得 forward 的 tensor\n",
    "        return grad_output * input  # 定义 dfun1/dinput = input\n",
    "\n",
    "\n",
    "fun = fun1.apply\n",
    "x = torch.tensor([12])\n",
    "k = torch.full((), 0.5, requires_grad=True)\n",
    "b = torch.full((), 0.7, requires_grad=True)\n",
    "c = torch.full((), 0.2, requires_grad=True)\n",
    "y = k * fun(c * x) + b\n",
    "y.backward()\n",
    "print(c.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 $\\frac{\\partial fun}{\\partial p} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class fun1(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        out = input ** 2  # 定义反向传播函数 fun1 = input ** 2\n",
    "        return out\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1  # 定义 dfun1/dinput = 1\n",
    "\n",
    "\n",
    "fun = fun1.apply\n",
    "x = torch.tensor([12])\n",
    "k = torch.full((), 0.5, requires_grad=True)\n",
    "b = torch.full((), 0.7, requires_grad=True)\n",
    "c = torch.full((), 0.2, requires_grad=True)\n",
    "y = k * fun(c * x) + b\n",
    "y.backward()\n",
    "print(c.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
